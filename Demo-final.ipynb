{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import imutils\n",
    "from imutils.video import VideoStream, WebcamVideoStream, FPS\n",
    "import pyautogui\n",
    "from collections import deque\n",
    "from PIL import Image\n",
    "\n",
    "queue_size = 32  # size of queue to retain for 3D conv input\n",
    "stable_queue_size = 4 # size of queue for prediction stabilisation\n",
    "num_classes = 27\n",
    "threshold = 0.25 # above the threshold will be recognized as gesture\n",
    "verbose = 1\n",
    "rgb_mean = (0.485, 0.456, 0.406)\n",
    "rgb_std = (0.229, 0.224, 0.225)\n",
    "expand = 1\n",
    "resolution = (96 * expand, 160* expand)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.CenterCrop((96,160)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(rgb_mean,rgb_std)\n",
    "    ])\n",
    "\n",
    "MobileFaceNet_BottleNeck_Setting = [\n",
    "    # t, c , n ,s\n",
    "    [2, 64, 5, 1],\n",
    "    [4, 128, 1, 1],\n",
    "    [2, 128, 6, 2],\n",
    "    [4, 128, 1, 2],\n",
    "    [2, 128, 2, 2]\n",
    "]\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expansion):\n",
    "        super(BottleNeck, self).__init__()\n",
    "        self.connect = stride == 1 and inp == oup\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # 1*1 conv\n",
    "            nn.Conv3d(inp, inp * expansion, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm3d(inp * expansion),\n",
    "            nn.PReLU(inp * expansion),\n",
    "\n",
    "            # 3*3 depth wise conv\n",
    "            nn.Conv3d(inp * expansion, inp * expansion, 3, stride, 1, groups=inp * expansion, bias=False),\n",
    "            nn.BatchNorm3d(inp * expansion),\n",
    "            nn.PReLU(inp * expansion),\n",
    "\n",
    "            # 1*1 conv\n",
    "            nn.Conv3d(inp * expansion, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm3d(oup),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, inp, oup, k, s, p, depthwidth=False, linear=False):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.linear = linear\n",
    "        if depthwidth:\n",
    "            self.conv = nn.Conv3d(inp, oup, k, s, p, groups=inp, bias=False)\n",
    "        else:\n",
    "            self.conv = nn.Conv3d(inp, oup, k, s, p, bias=False)\n",
    "\n",
    "        self.bn = nn.BatchNorm3d(oup)\n",
    "        if not linear:\n",
    "            self.prelu = nn.PReLU(oup)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.linear:\n",
    "            return x\n",
    "        else:\n",
    "            return self.prelu(x)\n",
    "\n",
    "class MobileFaceNet(nn.Module):\n",
    "    def __init__(self, feature_dim=256, num_classes=27, bottleneck_setting=MobileFaceNet_BottleNeck_Setting):\n",
    "        super(MobileFaceNet, self).__init__()\n",
    "        self.conv1 = ConvBlock(3, 64, (3, 7, 7), (1, 2, 2), (1, 3, 3))\n",
    "        self.dw_conv1 = ConvBlock(64, 64, 3, 2, 1, depthwidth=True)\n",
    "\n",
    "        self.cur_channel = 64\n",
    "        block = BottleNeck\n",
    "        self.blocks = self._make_layer(block, bottleneck_setting)\n",
    "\n",
    "        self.conv2 = ConvBlock(128, 512, 1, 1, 0)\n",
    "        self.linear7 = ConvBlock(512, 512, (1, 3, 5), 1, 0, depthwidth=True, linear=True)\n",
    "        self.linear1 = ConvBlock(512, feature_dim, 1, 1, 0, linear=True)\n",
    "        self.bn = nn.BatchNorm3d(feature_dim)\n",
    "        self.out = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(layer, nn.BatchNorm3d):\n",
    "                nn.init.constant_(layer.weight, val=1.0)\n",
    "                nn.init.constant_(layer.bias, val=0.0)\n",
    "            elif isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.constant_(layer.bias, val=0.0)\n",
    "\n",
    "    def _make_layer(self, block, setting):\n",
    "        layers = []\n",
    "        for t, c, n, s in setting:\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    layers.append(block(self.cur_channel, c, s, t))\n",
    "                else:\n",
    "                    layers.append(block(self.cur_channel, c, 1, t))\n",
    "                self.cur_channel = c\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.dw_conv1(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.linear7(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.bn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "#load the model\n",
    "model = MobileFaceNet()\n",
    "model = nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('project_classifier_mobile9228.pth', map_location=torch.device('cpu')))\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "\n",
    "gesture_dict = {\n",
    "     'Swiping Left': 0, 0: 'Swiping Left',\n",
    "     'Swiping Right': 1, 1: 'Swiping Right',\n",
    "     'Swiping Down': 2, 2: 'Swiping Down',\n",
    "     'Swiping Up': 3, 3: 'Swiping Up',\n",
    "     'Pushing Hand Away': 4, 4: 'Pushing Hand Away',\n",
    "     'Pulling Hand In': 5, 5: 'Pulling Hand In',\n",
    "     'Sliding Two Fingers Left': 6, 6: 'Sliding Two Fingers Left',\n",
    "     'Sliding Two Fingers Right': 7, 7: 'Sliding Two Fingers Right',\n",
    "     'Sliding Two Fingers Down': 8, 8: 'Sliding Two Fingers Down',\n",
    "     'Sliding Two Fingers Up': 9, 9: 'Sliding Two Fingers Up',\n",
    "     'Pushing Two Fingers Away': 10, 10: 'Pushing Two Fingers Away',\n",
    "     'Pulling Two Fingers In': 11, 11: 'Pulling Two Fingers In',\n",
    "     'Rolling Hand Forward': 12, 12: 'Rolling Hand Forward',\n",
    "     'Rolling Hand Backward': 13, 13: 'Rolling Hand Backward',\n",
    "     'Turning Hand Clockwise': 14, 14: 'Turning Hand Clockwise',\n",
    "     'Turning Hand Counterclockwise': 15, 15: 'Turning Hand Counterclockwise',\n",
    "     'Zooming In With Full Hand': 16, 16: 'Zooming In With Full Hand',\n",
    "     'Zooming Out With Full Hand': 17, 17: 'Zooming Out With Full Hand',\n",
    "     'Zooming In With Two Fingers': 18, 18: 'Zooming In With Two Fingers',\n",
    "     'Zooming Out With Two Fingers': 19, 19: 'Zooming Out With Two Fingers',\n",
    "     'Thumb Up': 20, 20: 'Thumb Up',\n",
    "     'Thumb Down': 21, 21: 'Thumb Down',\n",
    "     'Shaking Hand': 22, 22: 'Shaking Hand',\n",
    "     'Stop Sign':23, 23: 'Stop Sign',\n",
    "     'Drumming Fingers': 24, 24: 'Drumming Fingers',\n",
    "     'No gesture': 25, 25: 'No gesture',\n",
    "     'Doing other things': 26, 26: 'Doing other things'\n",
    "}\n",
    "\n",
    "action_dict = {\n",
    "     'Swiping Left': (1, \"left\"), \n",
    "     'Swiping Right': (1, \"right\"), \n",
    "     'Swiping Down': (1, \"down\"), \n",
    "     'Swiping Up': (1, \"up\"), \n",
    "     'Pushing Hand Away': (4, 'volumedown'), \n",
    "     'Pulling Hand In': (4, 'volumeup'), \n",
    "     'Sliding Two Fingers Left': (1, \"left\"),\n",
    "     'Sliding Two Fingers Right': (1, \"right\"), \n",
    "     'Sliding Two Fingers Down': (1, \"down\"), \n",
    "     'Sliding Two Fingers Up': (1, \"up\"), \n",
    "     'Pushing Two Fingers Away': (4, 'volumedown'),\n",
    "     'Pulling Two Fingers In': (4, 'volumeup'),\n",
    "     'Rolling Hand Forward': 0, \n",
    "     'Rolling Hand Backward': 0, \n",
    "     'Turning Hand Clockwise': 0, \n",
    "     'Turning Hand Counterclockwise': 0, \n",
    "     'Zooming In With Full Hand': (2, 'ctrl', \"+\"), \n",
    "     'Zooming Out With Full Hand': (2, 'ctrl', \"-\"), \n",
    "     'Zooming In With Two Fingers': (2, 'ctrl', \"+\"), \n",
    "     'Zooming Out With Two Fingers': (2, 'ctrl', \"-\"), \n",
    "     'Thumb Up': 0, \n",
    "     'Thumb Down': 0,\n",
    "     'Shaking Hand': 0, \n",
    "     'Stop Sign':(3, 'space'), \n",
    "     'Drumming Fingers': 0, \n",
    "     'No gesture': 0,\n",
    "     'Doing other things': 0,\n",
    "}\n",
    "\n",
    "\n",
    "def do_it(action_code):\n",
    "    if action_code != 0:\n",
    "        # moving pic\n",
    "        if action_code[0] == 1:\n",
    "#             pyautogui.keyDown(action_code[1])\n",
    "#             starttime = time.time()\n",
    "#             endtime = time.time()\n",
    "#             while endtime - starttime < 0.5:\n",
    "#                 endtime = time.time()\n",
    "#             pyautogui.keyUp(action_code[1])\n",
    "            \n",
    "#             pyautogui.click()\n",
    "            pyautogui.hotkey(action_code[1])\n",
    "            \n",
    "        elif action_code[0] == 2:\n",
    "            pyautogui.click()\n",
    "            pyautogui.hotkey(action_code[1], action_code[2])\n",
    "        \n",
    "        elif action_code[0] == 3:\n",
    "            pyautogui.press(action_code[1])\n",
    "        \n",
    "        elif action_code[0] == 4:\n",
    "            pyautogui.press(action_code[1])\n",
    "\n",
    "\n",
    "\n",
    "if verbose>0: print(\"[INFO] Attemping to start video stream...\")\n",
    "vs = VideoStream(0, usePiCamera=False, resolution=resolution, framerate=12).start()\n",
    "\n",
    "time.sleep(2.0)\n",
    "fps = FPS().start()\n",
    "Queue = deque(maxlen=queue_size)\n",
    "Stable_Queue = deque(maxlen=stable_queue_size)\n",
    "\n",
    "# act = deque(['No gesture', \"No gesture\"], maxlen=3)\n",
    "\n",
    "# read the frames from video stream\n",
    "frame = vs.read()\n",
    "\n",
    "if frame is None:\n",
    "    print('[ERROR] No video stream is available')\n",
    "\n",
    "else:\n",
    "# initialize the queue with the first frame\n",
    "    for i in range(queue_size):\n",
    "        Queue.append(frame)\n",
    "    if (verbose > 0): print('[INFO] Video stream started...')\n",
    "\n",
    "# Action candidate pool\n",
    "candidate_pool = []\n",
    "\n",
    "# loop over the frames to get sample video\n",
    "while (True):\n",
    "    # read the frames from video stream\n",
    "    frame = vs.read()\n",
    "    if frame is None:\n",
    "        print('[ERROR] No video stream is available')\n",
    "        break\n",
    "\n",
    "    # resize maximum height of 100 pixels (jester v1 dataset video height)\n",
    "\n",
    "    frame = imutils.resize(frame, height=100)\n",
    "    \n",
    "    raw_frame = frame.copy()\n",
    "    \n",
    "    Queue.append(frame)\n",
    "\n",
    "    # format data to torch\n",
    "    \n",
    "#     imgs = []\n",
    "#     for img_beforetransform in Queue:\n",
    "#         img = transform(img_beforetransform)\n",
    "# #         img2 = transform2(img1)\n",
    "# #         cv2.imshow('sample', np.array(img2))\n",
    "#         imgs.append(torch.unsqueeze(img, 0))\n",
    "#     video = torch.cat(imgs)\n",
    "    video = torch.zeros(queue_size, 3, 96, 160)\n",
    "    for i in range(queue_size):\n",
    "        video[i] = transform(Queue[i])\n",
    "    data = video.permute(1, 0, 2, 3).unsqueeze(0)\n",
    "    data = data[:,:,0:32:2,:,:].to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(data)\n",
    "    output = F.softmax(output, dim=1)\n",
    "\n",
    "    k = 5\n",
    "    ts, pred = output.detach().cpu().topk(k, 1, True, True)\n",
    "\n",
    "    top5 = pred[0]\n",
    "    ps = ts[0]\n",
    "    top1 = top5[0] if ps[0] > threshold else 26\n",
    "\n",
    "    hist = {}\n",
    "    for i in range(num_classes):\n",
    "        hist[i] = 0\n",
    "    for i in range(k):\n",
    "        hist[top5[i].item()] = ps[i].item()\n",
    "\n",
    "    Stable_Queue.append(list(hist.values()))\n",
    "\n",
    "    ave_pred = np.array(Stable_Queue).mean(axis=0)\n",
    "    top1 = np.argmax(ave_pred) if max(ave_pred) > threshold else 26\n",
    "    \n",
    "    cv2.imshow('test', raw_frame)\n",
    "    \n",
    "    if top1 != 25 and top1 != 26:\n",
    "        candidate_pool.append(top1)\n",
    "    num_candidate = len(candidate_pool)\n",
    "    \n",
    "    action = None\n",
    "    if top1 == 25 or top1 == 26:\n",
    "        trimed_candidate_pool = candidate_pool[int(0.2*num_candidate):int(0.85*num_candidate)]\n",
    "        num = len(trimed_candidate_pool)\n",
    "        if num != 0:\n",
    "#             print(trimed_candidate_pool)\n",
    "            if num <= 7:\n",
    "                candidate_pool = []\n",
    "            else:\n",
    "                action_index = max(set(trimed_candidate_pool), key=trimed_candidate_pool.count)\n",
    "                action = gesture_dict[action_index]\n",
    "                print(action)\n",
    "                candidate_pool = []\n",
    "    \n",
    "    if action is not None:\n",
    "        do_it(action_dict[action])\n",
    "                \n",
    "    \n",
    "#     ts, pred = output.detach().cpu().topk(k, 1, True, True)\n",
    "#     top5 = [gesture_dict[pred[0][i].item()] for i in range(k)]\n",
    "\n",
    "#     pi = [pred[0][i].item() for i in range(k)]\n",
    "#     ps = [ts[0][i].item() for i in range(k)]\n",
    "#     top1 = top5[0] if ps[0] > threshold else gesture_dict[26]\n",
    "\n",
    "#     hist = {}\n",
    "#     for i in range(num_classes):\n",
    "#         hist[i] = 0\n",
    "#     for i in range(len(pi)):\n",
    "#         hist[pi[i]] = ps[i]\n",
    "#         print(pi[i])\n",
    "#         print(ps[i])\n",
    "\n",
    "#     Stable_Queue.append(list(hist.values()))\n",
    "\n",
    "#     ave_pred = np.array(Stable_Queue).mean(axis=0)\n",
    "\n",
    "#     top1 = gesture_dict[np.argmax(ave_pred)] if max(ave_pred) > threshold else gesture_dict[26]\n",
    "#     cv2.imshow('test', raw_frame)\n",
    "#     top1 = top1.lower()\n",
    "    \n",
    "#     if top1 != 'doing other things' and top1 != 'no gesture':\n",
    "#         candidate_pool.append(top1)\n",
    "#     num_candidate = len(candidate_pool)\n",
    "    \n",
    "#     if top1 == 'doing other things' or top1 == 'no gesture':\n",
    "#         trimed_candidate_pool = candidate_pool[int(0.15*num_candidate):int(0.85*num_candidate)]\n",
    "\n",
    "#         if len(trimed_candidate_pool) != 0:\n",
    "#             print(trimed_candidate_pool)\n",
    "#             action = max(set(trimed_candidate_pool), key=trimed_candidate_pool.count)\n",
    "#             print(action)\n",
    "#             candidate_pool = []\n",
    "            \n",
    "\n",
    "    # if (act[0] != act[1] and len(set(list(act)[1:])) == 1):\n",
    "    #     if top1 in action.keys():\n",
    "    #         t = action[top1]['fn']\n",
    "    #         k = action[top1]['keys']\n",
    "    #\n",
    "    #         if verbose > 1: print('[DEBUG]', top1, '-- ', t, str(k))\n",
    "    #         if t == 'typewrite':\n",
    "    #             pyautogui.typewrite(k)\n",
    "    #         elif t == 'press':\n",
    "    #             pyautogui.press(k)\n",
    "    #         elif t == 'hotkey':\n",
    "    #             for key in k:\n",
    "    #                 pyautogui.keyDown(key)\n",
    "    #             for key in k[::-1]:\n",
    "    #                 pyautogui.keyUp(key)\n",
    "                # pyautogui.hotkey(\",\".join(k))\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "    # update the FPS counter\n",
    "    fps.update()\n",
    "\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "# cleaning up\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
