{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'D:/Jester/Jester/20bn-jester-v1/*'\n",
    "num_classes = 27\n",
    "num_worker = 0\n",
    "batch_size = 64\n",
    "scales = [1, 1/2**(1/4), 1/2**(1/2)]\n",
    "sample_size = (96,160)\n",
    "sample_duration = 16\n",
    "rgb_mean = (0.485, 0.456, 0.406)\n",
    "rgb_std = (0.229, 0.224, 0.225)\n",
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortKeyFunc(s):\n",
    "    return int(os.path.basename(s)[:])\n",
    "\n",
    "def load_all_path(root):\n",
    "    video_dictionary = glob.glob(root)\n",
    "    video_dictionary.sort(key=sortKeyFunc)\n",
    "    all_path = []\n",
    "    for video_path in video_dictionary:\n",
    "        file_list = sorted(glob.glob(video_path + '/*'))\n",
    "        all_path.append(file_list)\n",
    "    return all_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_path = load_all_path(root)\n",
    "labels = np.genfromtxt('D:/Jester/jester-v1-labels.csv', delimiter=',', dtype=np.str)\n",
    "labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalCrop(object):\n",
    "    \"\"\"Temporally crop the given frame indices at a random location or at the center location.\n",
    "        size (int): Desired output size of the crop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, mode):\n",
    "        self.size = size*2\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            paths (list): paths to be cropped.\n",
    "        Returns:\n",
    "            list: Cropped paths.\n",
    "        \"\"\"\n",
    "        num_frames = len(path)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            if num_frames < self.size:\n",
    "                num_loops = self.size//num_frames\n",
    "                delta = self.size - num_frames*num_loops\n",
    "                new_path = path*num_loops + path[0:delta]\n",
    "            else:\n",
    "                begin_index = random.randint(0, num_frames - self.size)\n",
    "                end_index = begin_index + self.size\n",
    "                new_path = path[begin_index:end_index]\n",
    "        else:\n",
    "            if num_frames < self.size:\n",
    "                num_loops = self.size//num_frames\n",
    "                delta = self.size - num_frames*num_loops\n",
    "                new_path = path*num_loops + path[0:delta]\n",
    "            else:\n",
    "                begin_index = (num_frames - self.size)//2\n",
    "                end_index = begin_index + self.size\n",
    "                new_path = path[begin_index:end_index]\n",
    "                \n",
    "        new_path = new_path[0:self.size:2]\n",
    "        return new_path\n",
    "\n",
    "    \n",
    "class MultiScaleRandomCrop(object):\n",
    "\n",
    "    def __init__(self, scales, size, interpolation=Image.BILINEAR):\n",
    "        self.scales = scales\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation       \n",
    "\n",
    "    def get_random_param(self):\n",
    "        self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n",
    "        self.topleft_x = random.random()\n",
    "        self.topleft_y = random.random()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        image_width, image_height = img.size\n",
    "        out_height, out_width = self.size\n",
    "        crop_height = out_height*self.scale\n",
    "        crop_width = out_width*self.scale\n",
    "\n",
    "        topleft_x = self.topleft_x * (image_width - crop_width)\n",
    "        topleft_y = self.topleft_y * (image_height - crop_height)\n",
    "        bottomright_x = topleft_x + crop_width\n",
    "        bottomright_y = topleft_y + crop_height\n",
    "\n",
    "        img = img.crop((topleft_x, topleft_y, bottomright_x, bottomright_y))\n",
    "        img = img.resize((out_width, out_height), self.interpolation)\n",
    "\n",
    "        return img\n",
    "    \n",
    "class RandomCrop(object):\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.size = size  \n",
    "\n",
    "    def get_random_param(self):\n",
    "        self.topleft_x = random.random()\n",
    "        self.topleft_y = random.random()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        image_width, image_height = img.size\n",
    "        out_height, out_width = self.size\n",
    "\n",
    "        topleft_x = self.topleft_x * (image_width - out_width)\n",
    "        topleft_y = self.topleft_y * (image_height - out_height)\n",
    "        bottomright_x = topleft_x + out_width\n",
    "        bottomright_y = topleft_y + out_height\n",
    "\n",
    "        img = img.crop((topleft_x, topleft_y, bottomright_x, bottomright_y))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video(paths, mode):\n",
    "    all_image = []\n",
    "    temporal_transform = TemporalCrop(sample_duration, mode)\n",
    "    if mode == 'train':\n",
    "        RandomCrops = MultiScaleRandomCrop(scales, sample_size)\n",
    "#         RandomCrops = RandomCrop(sample_size)\n",
    "        RandomCrops.get_random_param()\n",
    "        spatial_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            RandomCrops,\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(rgb_mean, rgb_std)\n",
    "        ])\n",
    "    else:\n",
    "        spatial_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "#             transforms.Resize(sample_size),\n",
    "            transforms.CenterCrop(sample_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(rgb_mean, rgb_std)\n",
    "        ])\n",
    "                \n",
    "    new_paths = temporal_transform(paths)\n",
    "    width = cv2.imread(new_paths[0]).shape[1]\n",
    "    if width != 176:\n",
    "        padding = np.zeros((100,(176-width)//2,3), dtype=np.uint8)\n",
    "        for path in new_paths:\n",
    "            image = cv2.imread(path)\n",
    "            image = np.concatenate([padding, image, padding], axis=1)\n",
    "            image = spatial_transform(image)\n",
    "            all_image.append(image)\n",
    "    else:\n",
    "        for path in new_paths:\n",
    "            image = cv2.imread(path)\n",
    "            image = spatial_transform(image)\n",
    "            all_image.append(image)\n",
    "            \n",
    "    video = np.stack(all_image).transpose(1,0,2,3)\n",
    "#     print(video.shape)\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, all_path, x, y, mode):\n",
    "        self.length = len(x)\n",
    "        self.all_path = all_path\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.mode = mode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(self.length)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'train' or self.mode == 'valid':\n",
    "            x = read_video(self.all_path[int(self.x[index,0])-1], self.mode)\n",
    "            y = int(np.argwhere(self.y == self.x[index,1]))\n",
    "            return torch.from_numpy(x), torch.tensor(y)\n",
    "        else:\n",
    "            x = read_video(self.all_path[int(self.x[index])-1], self.mode)\n",
    "            return torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.genfromtxt('D:/Jester/jester-v1-train.csv', delimiter=',', dtype=np.str)    \n",
    "train_data = Dataset(all_path, train, labels, 'train')\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_worker, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = np.genfromtxt('D:/Jester/jester-v1-validation.csv', delimiter=',', dtype=np.str)\n",
    "valid_data = Dataset(all_path, valid, labels, 'valid')\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, num_workers=num_worker, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.genfromtxt('D:/Jester/jester-v1-test.csv', delimiter=',', dtype=np.str)\n",
    "test_data = Dataset(all_path, test, labels, 'test')\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False, num_workers=num_worker, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalContextBlock(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=16):\n",
    "        super(GlobalContextBlock, self).__init__()\n",
    "\n",
    "        self.conv_mask = nn.Conv3d(in_channels, 1, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "        self.channel_add_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, in_channels//ratio, kernel_size=1),\n",
    "            nn.LayerNorm([in_channels//ratio, 1, 1, 1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels//ratio, in_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, time, height, width = x.size()\n",
    "\n",
    "        input_x = x \n",
    "        input_x = input_x.view(batch, channel, time * height * width) # [N, C, T * H * W]\n",
    "        \n",
    "        context_mask = self.conv_mask(x) # [N, 1, T, H, W]\n",
    "        context_mask = context_mask.view(batch, 1, time * height * width) # [N, 1, T * H * W]\n",
    "        context_mask = self.softmax(context_mask) # [N, 1, T * H * W]\n",
    "        context_mask = context_mask.squeeze(1).unsqueeze(2) # [N, T * H * W, 1]\n",
    "        context = torch.bmm(input_x, context_mask) # [N, C, 1]\n",
    "        \n",
    "        context = context.view(batch, channel, 1, 1, 1) # [N, C, 1, 1, 1]\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = self.spatial_pool(x)\n",
    "        channel_add_term = self.channel_add_conv(context)\n",
    "        out = x + channel_add_term\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MobileFaceNet_BottleNeck_Setting = [\n",
    "    # t, c , n ,s\n",
    "    [2, 128, 5, 1],\n",
    "    [4, 128, 1, 1],\n",
    "    [2, 128, 6, 2],\n",
    "    [4, 128, 1, 2],\n",
    "    [2, 128, 2, 2]\n",
    "]\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expansion):\n",
    "        super(BottleNeck, self).__init__()\n",
    "        self.connect = stride == 1 and inp == oup\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # 1*1 conv\n",
    "            nn.Conv3d(inp, inp * expansion, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm3d(inp * expansion),\n",
    "            nn.PReLU(inp * expansion),\n",
    "\n",
    "            # 3*3 depth wise conv\n",
    "            nn.Conv3d(inp * expansion, inp * expansion, 3, stride, 1, groups=inp * expansion, bias=False),\n",
    "            nn.BatchNorm3d(inp * expansion),\n",
    "            nn.PReLU(inp * expansion),\n",
    "\n",
    "            # 1*1 conv\n",
    "            nn.Conv3d(inp * expansion, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm3d(oup),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, inp, oup, k, s, p, dw=False, linear=False):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.linear = linear\n",
    "        if dw:\n",
    "            self.conv = nn.Conv3d(inp, oup, k, s, p, groups=inp, bias=False)\n",
    "        else:\n",
    "            self.conv = nn.Conv3d(inp, oup, k, s, p, bias=False)\n",
    "\n",
    "        self.bn = nn.BatchNorm3d(oup)\n",
    "        if not linear:\n",
    "            self.prelu = nn.PReLU(oup)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.linear:\n",
    "            return x\n",
    "        else:\n",
    "            return self.prelu(x)\n",
    "\n",
    "\n",
    "class MobileFaceNet(nn.Module):\n",
    "    def __init__(self, feature_dim=256, num_classes=27, bottleneck_setting=MobileFaceNet_BottleNeck_Setting):\n",
    "        super(MobileFaceNet, self).__init__()\n",
    "        self.conv1 = ConvBlock(3, 64, (3,7,7), (1,2,2), (1,3,3))\n",
    "        self.dw_conv1 = ConvBlock(64, 128, 3, 2, 1, dw=True)\n",
    "\n",
    "        self.cur_channel = 128\n",
    "        block = BottleNeck\n",
    "        self.blocks = self._make_layer(block, bottleneck_setting)\n",
    "\n",
    "        self.conv2 = ConvBlock(128, 512, 1, 1, 0)\n",
    "        self.linear7 = ConvBlock(512, 512, (1,3,5), 1, 0, dw=True, linear=True)\n",
    "        self.linear1 = ConvBlock(512, feature_dim, 1, 1, 0, linear=True)\n",
    "        self.bn = nn.BatchNorm3d(feature_dim)\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "        self.out = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(layer, nn.BatchNorm3d):\n",
    "                nn.init.constant_(layer.weight, val=1.0)\n",
    "                nn.init.constant_(layer.bias, val=0.0)\n",
    "            elif isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.constant_(layer.bias, val=0.0)\n",
    "\n",
    "    \n",
    "    def _make_layer(self, block, setting):\n",
    "        layers = []\n",
    "        for t, c, n, s in setting:\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    layers.append(block(self.cur_channel, c, s, t))\n",
    "                    layers.append(GlobalContextBlock(self.cur_channel))\n",
    "#                 elif i != n-1:\n",
    "#                     layers.append(block(self.cur_channel, c, 1, t))\n",
    "#                     layers.append(GlobalContextBlock(self.cur_channel))\n",
    "                else:\n",
    "                    layers.append(block(self.cur_channel, c, 1, t))\n",
    "                    layers.append(GlobalContextBlock(self.cur_channel))\n",
    "                self.cur_channel = c\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.dw_conv1(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.linear7(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.bn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         x = self.dropout(x)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "    \n",
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_predictions = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(i, loss)\n",
    "\n",
    "    running_loss /= len(train_loader)\n",
    "    acc = (correct_predictions/total_predictions)*100.0\n",
    "    print('Training Loss: ', running_loss)\n",
    "    print('Training Accuracy: ', acc, '%')\n",
    "    return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        wrong = 0\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "#             for n in range(labels.size(0)):\n",
    "#                 if predicted[n] != labels[n]:\n",
    "#                     print('Prediction: ',predicted[n], '////Groundtruth: ', labels[n])\n",
    "#                     if int(labels[n].cpu()) in [0,1,2,3,6,7,8,9,14,15,16,17,18,19,23,25]:\n",
    "#                         wrong +=1\n",
    "#             print(i,wrong)\n",
    "            \n",
    "            loss = criterion(outputs, labels).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Validation Loss: ', running_loss)\n",
    "        print('Validation Accuracy: ', acc, '%')\n",
    "        return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileFaceNet()\n",
    "model = nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('project_classifier_mobile_gc9228.pth'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor=0.1, patience=2)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 100\n",
    "# Train_loss = []\n",
    "# Train_acc = []\n",
    "# Valid_loss = []\n",
    "# Valid_acc = []\n",
    "# num_no_improve = 0\n",
    "# for i in range(n_epochs):\n",
    "#     train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "#     valid_loss, valid_acc = test_model(model, valid_loader, criterion)\n",
    "#     Train_acc.append(train_acc)\n",
    "#     Train_loss.append(train_loss)\n",
    "#     Valid_loss.append(valid_loss)\n",
    "#     scheduler.step(valid_loss)\n",
    "#     print('='*40)\n",
    "\n",
    "#     if i == 0:\n",
    "#         torch.save(model.state_dict(), 'project_classifier_mobile_gc.pth')\n",
    "#     else:\n",
    "#         if valid_acc > max(Valid_acc):\n",
    "#             torch.save(model.state_dict(), 'project_classifier_mobile_gc.pth')\n",
    "#             num_no_improve = 0\n",
    "#         else:\n",
    "#             num_no_improve += 1\n",
    "#     Valid_acc.append(valid_acc)\n",
    "    \n",
    "#     training_loss = np.array(Train_loss).reshape(-1,1)\n",
    "#     training_acc = np.array(Train_acc).reshape(-1,1)\n",
    "#     validation_loss = np.array(Valid_loss).reshape(-1,1)\n",
    "#     validation_acc = np.array(Valid_acc).reshape(-1,1)\n",
    "#     result = np.concatenate([training_loss, training_acc, validation_loss, validation_acc], axis=1)\n",
    "#     np.savetxt('result_project_classifier_mobile_gc.csv', result, delimiter=',', fmt='%1.5f', header='training_loss,training_acc,validation_loss,validation_acc', comments='')\n",
    "    \n",
    "#     if num_no_improve >= 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        output = []\n",
    "        total_duration = 0\n",
    "        labels = np.genfromtxt('D:/Jester/jester-v1-labels.csv', delimiter=',', dtype=np.str)\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            start = time.time()\n",
    "            inputs = data.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            end = time.time()\n",
    "            \n",
    "            for n in range(inputs.size(0)):\n",
    "                prediction = labels[predicted[n]]\n",
    "                output.append(prediction)\n",
    "            \n",
    "            duration = end - start\n",
    "            total_duration += duration\n",
    "            print(i, duration)\n",
    "        \n",
    "        index = np.genfromtxt('D:/Jester/jester-v1-test.csv', delimiter=',', dtype=np.str).reshape(-1,1)\n",
    "        Predicted = np.array(output, dtype=np.str).reshape(-1,1)\n",
    "        submission = np.concatenate((index, Predicted), axis=1)\n",
    "        \n",
    "    np.savetxt('predict.csv', Predicted, delimiter=',', fmt='%s')\n",
    "    average_duration = total_duration/len(test_loader.dataset)\n",
    "    print(average_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
