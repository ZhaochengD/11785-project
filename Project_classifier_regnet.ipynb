{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'D:/Jester/Jester/20bn-jester-v1/*'\n",
    "num_classes = 27\n",
    "num_worker = 0\n",
    "batch_size = 16\n",
    "scales = [1, 1/2**(1/4), 1/2**(1/2)]\n",
    "sample_size = (96,160)\n",
    "sample_duration = 16\n",
    "num_channels = [144,288,576,1296]\n",
    "num_blocks = [2,7,14,2]\n",
    "cardinality = 72\n",
    "rgb_mean = (0.485, 0.456, 0.406)\n",
    "rgb_std = (0.229, 0.224, 0.225)\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortKeyFunc(s):\n",
    "    return int(os.path.basename(s)[:])\n",
    "\n",
    "def load_all_path(root):\n",
    "    video_dictionary = glob.glob(root)\n",
    "    video_dictionary.sort(key=sortKeyFunc)\n",
    "    all_path = []\n",
    "    for video_path in video_dictionary:\n",
    "        file_list = sorted(glob.glob(video_path + '/*'))\n",
    "        all_path.append(file_list)\n",
    "    return all_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_path = load_all_path(root)\n",
    "labels = np.genfromtxt('D:/Jester/jester-v1-labels.csv', delimiter=',', dtype=np.str)\n",
    "labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalCrop(object):\n",
    "    \"\"\"Temporally crop the given frame indices at a random location or at the center location.\n",
    "        size (int): Desired output size of the crop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, mode):\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            paths (list): paths to be cropped.\n",
    "        Returns:\n",
    "            list: Cropped paths.\n",
    "        \"\"\"\n",
    "        num_frames = len(path)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            if num_frames < self.size:\n",
    "                num_loops = self.size//num_frames\n",
    "                delta = self.size - num_frames*num_loops\n",
    "                new_path = path*num_loops + path[0:delta]\n",
    "            else:\n",
    "                begin_index = random.randint(0, num_frames - self.size)\n",
    "                end_index = begin_index + self.size\n",
    "                new_path = path[begin_index:end_index]\n",
    "        else:\n",
    "            if num_frames < self.size:\n",
    "                num_loops = self.size//num_frames\n",
    "                delta = self.size - num_frames*num_loops\n",
    "                new_path = path*num_loops + path[0:delta]\n",
    "            else:\n",
    "                begin_index = (num_frames - self.size)//2\n",
    "                end_index = begin_index + self.size\n",
    "                new_path = path[begin_index:end_index]\n",
    "        \n",
    "        return new_path\n",
    "\n",
    "    \n",
    "class MultiScaleRandomCrop(object):\n",
    "\n",
    "    def __init__(self, scales, size, interpolation=Image.BILINEAR):\n",
    "        self.scales = scales\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation       \n",
    "\n",
    "    def get_random_param(self):\n",
    "        self.scale = self.scales[random.randint(0, len(self.scales) - 1)]\n",
    "        self.topleft_x = random.random()\n",
    "        self.topleft_y = random.random()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        image_width, image_height = img.size\n",
    "        out_height, out_width = self.size\n",
    "        crop_height = out_height*self.scale\n",
    "        crop_width = out_width*self.scale\n",
    "\n",
    "        topleft_x = self.topleft_x * (image_width - crop_width)\n",
    "        topleft_y = self.topleft_y * (image_height - crop_height)\n",
    "        bottomright_x = topleft_x + crop_width\n",
    "        bottomright_y = topleft_y + crop_height\n",
    "\n",
    "        img = img.crop((topleft_x, topleft_y, bottomright_x, bottomright_y))\n",
    "        img = img.resize((out_width, out_height), self.interpolation)\n",
    "\n",
    "        return img\n",
    "\n",
    "class RandomCrop(object):\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.size = size  \n",
    "\n",
    "    def get_random_param(self):\n",
    "        self.topleft_x = random.random()\n",
    "        self.topleft_y = random.random()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        image_width, image_height = img.size\n",
    "        out_height, out_width = self.size\n",
    "\n",
    "        topleft_x = self.topleft_x * (image_width - out_width)\n",
    "        topleft_y = self.topleft_y * (image_height - out_height)\n",
    "        bottomright_x = topleft_x + out_width\n",
    "        bottomright_y = topleft_y + out_height\n",
    "\n",
    "        img = img.crop((topleft_x, topleft_y, bottomright_x, bottomright_y))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video(paths, mode):\n",
    "    all_image = []\n",
    "    temporal_transform = TemporalCrop(sample_duration, mode)\n",
    "    if mode == 'train':\n",
    "        RandomCrops = MultiScaleRandomCrop(scales, sample_size)\n",
    "        RandomCrops.get_random_param()\n",
    "        spatial_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            RandomCrops,\n",
    "    #         SpatialElasticDisplacement(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(rgb_mean, rgb_std)\n",
    "        ])\n",
    "    else:\n",
    "        spatial_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "#             transforms.Resize(sample_size),\n",
    "            transforms.CenterCrop(sample_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(rgb_mean, rgb_std)\n",
    "        ])\n",
    "                \n",
    "    new_paths = temporal_transform(paths)\n",
    "    width = cv2.imread(new_paths[0]).shape[1]\n",
    "    if width != 176:\n",
    "        padding = np.zeros((100,(176-width)//2,3), dtype=np.uint8)\n",
    "        for path in new_paths:\n",
    "            image = cv2.imread(path)\n",
    "            image = np.concatenate([padding, image, padding], axis=1)\n",
    "            image = spatial_transform(image)\n",
    "            all_image.append(image)\n",
    "    else:\n",
    "        for path in new_paths:\n",
    "            image = cv2.imread(path)\n",
    "            image = spatial_transform(image)\n",
    "            all_image.append(image)\n",
    "            \n",
    "    video = np.stack(all_image).transpose(1,0,2,3)\n",
    "#     print(video.shape)\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, all_path, x, y, mode):\n",
    "        self.length = len(x)\n",
    "        self.all_path = all_path\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.mode = mode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(self.length)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'train' or self.mode == 'valid':\n",
    "            x = read_video(self.all_path[int(self.x[index,0])-1], self.mode)\n",
    "            y = int(np.argwhere(self.y == self.x[index,1]))\n",
    "            return torch.from_numpy(x), torch.tensor(y)\n",
    "        else:\n",
    "            x = read_video(self.all_path[int(self.x[index])-1], self.mode)\n",
    "            return torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.genfromtxt('D:/Jester/jester-v1-train.csv', delimiter=',', dtype=np.str)    \n",
    "train_data = Dataset(all_path, train, labels, 'train')\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_worker, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = np.genfromtxt('D:/Jester/jester-v1-validation.csv', delimiter=',', dtype=np.str)\n",
    "valid_data = Dataset(all_path, valid, labels, 'valid')\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, num_workers=num_worker, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.genfromtxt('D:/Jester/jester-v1-test.csv', delimiter=',', dtype=np.str)\n",
    "test_data = Dataset(all_path, test, labels, 'test')\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=False, num_workers=num_worker, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegNetBlockY(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, cardinality=32, stride=1):\n",
    "        super(RegNetBlockY, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "        )\n",
    "        \n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            nn.Conv3d(out_channels, out_channels//4, kernel_size=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv3d(out_channels//4, out_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.prelu = nn.PReLU()\n",
    "        \n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(out_channels)                \n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        out = out*self.se(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = self.prelu(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class RegNet(nn.Module):\n",
    "    def __init__(self, block, num_channels, num_blocks, cardinality, num_class):\n",
    "        super(RegNet, self).__init__()\n",
    "        self.cardinality = cardinality\n",
    "\n",
    "        self.feature_detector = nn.Sequential(\n",
    "            nn.Conv3d(3, num_channels[0], kernel_size=(3,7,7), stride=(1,2,2), padding=(1,3,3), bias=False),\n",
    "            nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n",
    "            self.stack_blocks(num_channels[0], num_channels[1], block, num_blocks[0], stride=1),\n",
    "            self.stack_blocks(num_channels[1], num_channels[2], block, num_blocks[1], stride=2),\n",
    "            self.stack_blocks(num_channels[2], num_channels[3], block, num_blocks[2], stride=2),\n",
    "            self.stack_blocks(num_channels[3], num_channels[3]*2,block, num_blocks[3], stride=2),\n",
    "            nn.AvgPool3d((1,3,5)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(num_channels[3]*2,num_class)\n",
    "        \n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(layer, nn.BatchNorm3d):\n",
    "                nn.init.constant_(layer.weight, val=1.0)\n",
    "                nn.init.constant_(layer.bias, val=0.0)\n",
    "            elif isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.constant_(layer.bias, val=0.0)\n",
    "                \n",
    "    def stack_blocks(self, in_channels, out_channels, block, num_block, stride):\n",
    "        strides = [1]*(num_block-1)\n",
    "        layers = []\n",
    "        layers.append(block(in_channels, out_channels, self.cardinality, stride))\n",
    "        for stride in strides:\n",
    "            layers.append(block(out_channels, out_channels, self.cardinality, stride))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.feature_detector(x)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_predictions = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        print(i, loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss /= len(train_loader)\n",
    "    acc = (correct_predictions/total_predictions)*100.0\n",
    "    print('Training Loss: ', running_loss)\n",
    "    print('Training Accuracy: ', acc, '%')\n",
    "    return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        \n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, labels).detach()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Validation Loss: ', running_loss)\n",
    "        print('Validation Accuracy: ', acc, '%')\n",
    "        return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegNet(RegNetBlockY, num_channels, num_blocks, cardinality=cardinality, num_class=num_classes)\n",
    "model.load_state_dict(torch.load('project_classifier_regnet9046.pth'))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor=0.1, patience=1)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 100\n",
    "# Train_loss = []\n",
    "# Train_acc = []\n",
    "# Valid_loss = []\n",
    "# Valid_acc = []\n",
    "# num_no_improve = 0\n",
    "# for i in range(n_epochs):\n",
    "#     train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "#     valid_loss, valid_acc = test_model(model, valid_loader, criterion)\n",
    "#     Train_acc.append(train_acc)\n",
    "#     Train_loss.append(train_loss)\n",
    "#     Valid_loss.append(valid_loss)\n",
    "#     scheduler.step(valid_loss)\n",
    "#     print('='*40)\n",
    "\n",
    "#     if i == 0:\n",
    "#         torch.save(model.state_dict(), 'project_classifier_regnet.pth')\n",
    "#     else:\n",
    "#         if valid_acc > max(Valid_acc):\n",
    "#             torch.save(model.state_dict(), 'project_classifier_regnet.pth')\n",
    "#             num_no_improve = 0\n",
    "#         else:\n",
    "#             num_no_improve += 1\n",
    "#     Valid_acc.append(valid_acc)\n",
    "    \n",
    "#     training_loss = np.array(Train_loss).reshape(-1,1)\n",
    "#     training_acc = np.array(Train_acc).reshape(-1,1)\n",
    "#     validation_loss = np.array(Valid_loss).reshape(-1,1)\n",
    "#     validation_acc = np.array(Valid_acc).reshape(-1,1)\n",
    "#     result = np.concatenate([training_loss, training_acc, validation_loss, validation_acc], axis=1)\n",
    "#     np.savetxt('result_project_classifier_regnet.csv', result, delimiter=',', fmt='%1.5f', header='training_loss,training_acc,validation_loss,validation_acc', comments='')\n",
    "    \n",
    "#     if num_no_improve >= 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        output = []\n",
    "        total_duration = 0\n",
    "        labels = np.genfromtxt('D:/Jester/jester-v1-labels.csv', delimiter=',', dtype=np.str)\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            start = time.time()\n",
    "            inputs = data.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            end = time.time()\n",
    "            \n",
    "            for n in range(inputs.size(0)):\n",
    "                prediction = labels[predicted[n]]\n",
    "                output.append(prediction)\n",
    "            \n",
    "            duration = end - start\n",
    "            total_duration += duration\n",
    "            print(i, duration)\n",
    "        \n",
    "        index = np.genfromtxt('D:/Jester/jester-v1-test.csv', delimiter=',', dtype=np.str).reshape(-1,1)\n",
    "        Predicted = np.array(output, dtype=np.str).reshape(-1,1)\n",
    "        submission = np.concatenate((index, Predicted), axis=1)\n",
    "        \n",
    "    np.savetxt('predict.csv', Predicted, delimiter=',', fmt='%s')\n",
    "    average_duration = total_duration/len(test_loader.dataset)\n",
    "    print(average_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
